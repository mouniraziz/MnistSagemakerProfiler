{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7976f9f2-9e8a-44c0-8250-f11a4c66dd65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No gpu device available!\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "if device==torch.device(\"cpu\"):\n",
    "    print(\"No gpu device available!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb30787a-fa84-4a22-b766-bb94f6ebbe6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m#\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# Licensed under the Apache License, Version 2.0 (the \"License\"). You\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# may not use this file except in compliance with the License. A copy of\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# the License is located at\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m#\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m#     http://aws.amazon.com/apache2.0/\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m#\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# or in the \"license\" file accompanying this file. This file is\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# ANY KIND, either express or implied. See the License for the specific\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# language governing permissions and limitations under the License.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpackaging\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mversion\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Version\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctional\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mF\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mparallel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistributedDataParallel \u001b[34mas\u001b[39;49;00m DDP\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mlr_scheduler\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m StepLR\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datasets, transforms\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtensorboard\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SummaryWriter\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msmprof\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# Network definition\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel_def\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Net\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "SMProf = \u001b[34mNone\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# override dependency on mirrors provided by torch vision package\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# from torchvision 0.9.1, 2 candidate mirror website links will be added before \"resources\" items automatically\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# Reference PR: https://github.com/pytorch/vision/pull/3559\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "TORCHVISION_VERSION = \u001b[33m\"\u001b[39;49;00m\u001b[33m0.9.1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m Version(torchvision.__version__) < Version(TORCHVISION_VERSION):\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Set path to data source and include checksum key to make sure data isn't corrupted\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    datasets.MNIST.resources = [\u001b[37m\u001b[39;49;00m\n",
      "        (\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mhttps://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-images-idx3-ubyte.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mf68b3c2dcbeaaa9fbdd348bbdeb94873\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        ),\u001b[37m\u001b[39;49;00m\n",
      "        (\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mhttps://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-labels-idx1-ubyte.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33md53e105ee54ea40749a09fcbcd1e9432\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        ),\u001b[37m\u001b[39;49;00m\n",
      "        (\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mhttps://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-images-idx3-ubyte.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m9fb629c4189551a2d022fa330f9573f3\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        ),\u001b[37m\u001b[39;49;00m\n",
      "        (\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mhttps://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-labels-idx1-ubyte.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mec29112dd5afa0611ce80d1b7f02629c\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        ),\u001b[37m\u001b[39;49;00m\n",
      "    ]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Set path to data source\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    datasets.MNIST.mirrors = [\u001b[33m\"\u001b[39;49;00m\u001b[33mhttps://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(args, model, device, train_loader, optimizer, epoch):\u001b[37m\u001b[39;49;00m\n",
      "    model.train()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    SMProf.start_profiling()\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m batch_idx, (data, target) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_loader):\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mwith\u001b[39;49;00m smprof.annotate(\u001b[33m\"\u001b[39;49;00m\u001b[33mstep_\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m+\u001b[36mstr\u001b[39;49;00m(batch_idx)):     \u001b[37m\u001b[39;49;00m\n",
      "            data, target = data.to(device), target.to(device)\u001b[37m\u001b[39;49;00m\n",
      "            optimizer.zero_grad()\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mwith\u001b[39;49;00m smprof.annotate(\u001b[33m\"\u001b[39;49;00m\u001b[33mForward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m): \u001b[37m\u001b[39;49;00m\n",
      "                output = model(data)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mwith\u001b[39;49;00m smprof.annotate(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoss\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m): \u001b[37m\u001b[39;49;00m\n",
      "                loss = F.nll_loss(output, target)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mwith\u001b[39;49;00m smprof.annotate(\u001b[33m\"\u001b[39;49;00m\u001b[33mBackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m): \u001b[37m\u001b[39;49;00m\n",
      "                loss.backward()\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mwith\u001b[39;49;00m smprof.annotate(\u001b[33m\"\u001b[39;49;00m\u001b[33mOptimizer\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "                optimizer.step()\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mif\u001b[39;49;00m batch_idx % args.log_interval == \u001b[34m0\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[37m\u001b[39;49;00m\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mTrain Epoch: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m [\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m)]\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33mLoss: \u001b[39;49;00m\u001b[33m{:.6f}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\u001b[37m\u001b[39;49;00m\n",
      "                        epoch,\u001b[37m\u001b[39;49;00m\n",
      "                        batch_idx * \u001b[36mlen\u001b[39;49;00m(data),\u001b[37m\u001b[39;49;00m\n",
      "                        \u001b[36mlen\u001b[39;49;00m(train_loader.dataset),\u001b[37m\u001b[39;49;00m\n",
      "                        \u001b[34m100.0\u001b[39;49;00m * batch_idx / \u001b[36mlen\u001b[39;49;00m(train_loader),\u001b[37m\u001b[39;49;00m\n",
      "                        loss.item(),\u001b[37m\u001b[39;49;00m\n",
      "                    )\u001b[37m\u001b[39;49;00m\n",
      "                )\u001b[37m\u001b[39;49;00m\n",
      "    SMProf.stop_profiling()    \u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest\u001b[39;49;00m(model, device, test_loader):\u001b[37m\u001b[39;49;00m\n",
      "    model.eval()\u001b[37m\u001b[39;49;00m\n",
      "    test_loss = \u001b[34m0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    correct = \u001b[34m0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mfor\u001b[39;49;00m data, target \u001b[35min\u001b[39;49;00m test_loader:\u001b[37m\u001b[39;49;00m\n",
      "            data, target = data.to(device), target.to(device)\u001b[37m\u001b[39;49;00m\n",
      "            output = model(data)\u001b[37m\u001b[39;49;00m\n",
      "            test_loss += F.nll_loss(output, target, reduction=\u001b[33m\"\u001b[39;49;00m\u001b[33msum\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).item()  \u001b[37m# sum up batch loss\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            pred = output.argmax(dim=\u001b[34m1\u001b[39;49;00m, keepdim=\u001b[34mTrue\u001b[39;49;00m)  \u001b[37m# get the index of the max log-probability\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            correct += pred.eq(target.view_as(pred)).sum().item()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    test_loss /= \u001b[36mlen\u001b[39;49;00m(test_loader.dataset)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mTest set: Average loss: \u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33m, Accuracy: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m)\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\u001b[37m\u001b[39;49;00m\n",
      "            test_loss, correct, \u001b[36mlen\u001b[39;49;00m(test_loader.dataset), \u001b[34m100.0\u001b[39;49;00m * correct / \u001b[36mlen\u001b[39;49;00m(test_loader.dataset)\u001b[37m\u001b[39;49;00m\n",
      "        )\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Training settings\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser(description=\u001b[33m\"\u001b[39;49;00m\u001b[33mPyTorch MNIST Example\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34m128\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--test-batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34m1000\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33minput batch size for testing (default: 1000)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34m2\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 14)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m1.0\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning rate (default: 1.0)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--gamma\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34m0.7\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mLearning rate step gamma (default: 0.7)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mrandom seed (default: 1)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--log-interval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34m10\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mhow many batches to wait before logging training status\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--save-model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, action=\u001b[33m\"\u001b[39;49;00m\u001b[33mstore_true\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mFor Saving the current Model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--data-path\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[33m\"\u001b[39;49;00m\u001b[33m/tmp/data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mPath for downloading \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mthe MNIST dataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    args = parser.parse_args()\u001b[37m\u001b[39;49;00m\n",
      "    args.lr = \u001b[34m1.0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    args.batch_size = \u001b[36mmax\u001b[39;49;00m(args.batch_size, \u001b[34m1\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    data_path = args.data_path\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    torch.manual_seed(args.seed)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    train_dataset = datasets.MNIST(\u001b[37m\u001b[39;49;00m\n",
      "        data_path,\u001b[37m\u001b[39;49;00m\n",
      "        train=\u001b[34mTrue\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        download=\u001b[34mTrue\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        transform=transforms.Compose(\u001b[37m\u001b[39;49;00m\n",
      "            [transforms.ToTensor(), transforms.Normalize((\u001b[34m0.1307\u001b[39;49;00m,), (\u001b[34m0.3081\u001b[39;49;00m,))]\u001b[37m\u001b[39;49;00m\n",
      "        ),\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    train_loader = torch.utils.data.DataLoader(\u001b[37m\u001b[39;49;00m\n",
      "        train_dataset,\u001b[37m\u001b[39;49;00m\n",
      "        batch_size=args.batch_size,\u001b[37m\u001b[39;49;00m\n",
      "        shuffle=\u001b[34mFalse\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        num_workers=\u001b[34m0\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        pin_memory=\u001b[34mTrue\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    test_loader = torch.utils.data.DataLoader(\u001b[37m\u001b[39;49;00m\n",
      "        datasets.MNIST(\u001b[37m\u001b[39;49;00m\n",
      "            data_path,\u001b[37m\u001b[39;49;00m\n",
      "            train=\u001b[34mFalse\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "            transform=transforms.Compose(\u001b[37m\u001b[39;49;00m\n",
      "                [transforms.ToTensor(), transforms.Normalize((\u001b[34m0.1307\u001b[39;49;00m,), (\u001b[34m0.3081\u001b[39;49;00m,))]\u001b[37m\u001b[39;49;00m\n",
      "            ),\u001b[37m\u001b[39;49;00m\n",
      "        ),\u001b[37m\u001b[39;49;00m\n",
      "        batch_size=args.test_batch_size,\u001b[37m\u001b[39;49;00m\n",
      "        shuffle=\u001b[34mTrue\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    model = Net().to(device)\u001b[37m\u001b[39;49;00m\n",
      "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\u001b[37m\u001b[39;49;00m\n",
      "    scheduler = StepLR(optimizer, step_size=\u001b[34m1\u001b[39;49;00m, gamma=args.gamma)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mglobal\u001b[39;49;00m SMProf\u001b[37m\u001b[39;49;00m\n",
      "    SMProf = smprof.SMProfiler.instance()\u001b[37m\u001b[39;49;00m\n",
      "    config = smprof.Config()\u001b[37m\u001b[39;49;00m\n",
      "    config.profiler = {\u001b[37m\u001b[39;49;00m\n",
      "     \u001b[33m\"\u001b[39;49;00m\u001b[33mEnableCuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    }\u001b[37m\u001b[39;49;00m\n",
      "    SMProf.configure(config)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Create SummaryWriter only for the first rank of the first node\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, args.epochs + \u001b[34m1\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "        train(args, model, device, train_loader, optimizer, epoch)\u001b[37m\u001b[39;49;00m\n",
      "        test(model, device, test_loader)\u001b[37m\u001b[39;49;00m\n",
      "        scheduler.step()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m args.save_model:\u001b[37m\u001b[39;49;00m\n",
      "        torch.save(model.state_dict(), \u001b[33m\"\u001b[39;49;00m\u001b[33mmnist_cnn.pt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "    main()\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize Mnist_code/mnist_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4959f2b-9049-465b-802d-8f5efa597007",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import ProfilerConfig, Profiler\n",
    "profiler_config = ProfilerConfig(profile_params = Profiler(cpu_profiling_duration=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe61517c-84ba-4d2a-aff1-00ce0e73a07f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker import ProfilerConfig, Profiler\n",
    "\n",
    "from sagemaker.local import LocalSession\n",
    "import sagemaker\n",
    "\n",
    "# Creates a SageMaker session to interact with the AWS environment.\n",
    "sagemaker_session = LocalSession()\n",
    "# Set up the local SageMaker session\n",
    "local_session = LocalSession()\n",
    "local_session.config = {\"local\": {\"local_code\": True}}\n",
    "\n",
    "# Fetches the AWS region name for the current session.\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "# Retrieves the default Amazon S3 bucket for the SageMaker session.\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# Specifies a folder prefix within the bucket for organizing resources.\n",
    "prefix = \"sagemaker/DEMO-sagemaker-profiler\"\n",
    "\n",
    "# Fetches the IAM role ARN associated with the SageMaker notebook or instance.\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "instance_type=\"local\"\n",
    "env = {\n",
    "    \"SAGEMAKER_REQUIREMENTS\": \"requirements.txt\", \n",
    "}\n",
    "\n",
    "estimator = PyTorch(\n",
    "    base_job_name=\"Pytorch-mnist-profiler\",\n",
    "    source_dir=\"Mnist_code\",\n",
    "    entry_point=\"mnist_train.py\",\n",
    "    role=role,\n",
    "    framework_version=\"1.11.0\",\n",
    "    py_version=\"py38\",\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    sagemaker_session=local_session,  # Use the local session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e48836fd-1bae-46cc-82ff-9fa848af65f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: Mounir-pytorch-mnist-profiler-2024-11-28-04-53-56-121\n",
      "INFO:sagemaker.local.image:'Docker Compose' is not installed. Proceeding to check for 'docker-compose' CLI.\n",
      "INFO:sagemaker.local.image:'Docker Compose' found using Docker Compose CLI.\n",
      "INFO:sagemaker.local.local_session:Starting training job\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:sagemaker.local.image:No AWS credentials found in session but credentials from EC2 Metadata Service are available.\n",
      "INFO:sagemaker.local.image:docker compose file: \n",
      "networks:\n",
      "  sagemaker-local:\n",
      "    name: sagemaker-local\n",
      "services:\n",
      "  algo-1-uoa0q:\n",
      "    command: train\n",
      "    container_name: fzavvkcvok-algo-1-uoa0q\n",
      "    environment:\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    image: 763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:1.11.0-cpu-py38\n",
      "    networks:\n",
      "      sagemaker-local:\n",
      "        aliases:\n",
      "        - algo-1-uoa0q\n",
      "    stdin_open: true\n",
      "    tty: true\n",
      "    volumes:\n",
      "    - /tmp/tmphh2b8mx2/algo-1-uoa0q/output:/opt/ml/output\n",
      "    - /tmp/tmphh2b8mx2/algo-1-uoa0q/input:/opt/ml/input\n",
      "    - /tmp/tmphh2b8mx2/algo-1-uoa0q/output/data:/opt/ml/output/data\n",
      "    - /tmp/tmphh2b8mx2/model:/opt/ml/model\n",
      "    - /opt/ml/metadata:/opt/ml/metadata\n",
      "    - /home/ec2-user/SageMaker/git/fm-training/Mnist_code:/opt/ml/code\n",
      "    - /tmp/tmphh2b8mx2/shared:/opt/ml/shared\n",
      "version: '2.3'\n",
      "\n",
      "INFO:sagemaker.local.image:docker command: docker-compose -f /tmp/tmphh2b8mx2/docker-compose.yaml up --build --abort-on-container-exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Container fzavvkcvok-algo-1-uoa0q  Creating\n",
      " Container fzavvkcvok-algo-1-uoa0q  Created\n",
      "Attaching to fzavvkcvok-algo-1-uoa0q\n",
      "fzavvkcvok-algo-1-uoa0q  | 2024-11-27 23:53:57,403 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "fzavvkcvok-algo-1-uoa0q  | 2024-11-27 23:53:57,406 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "fzavvkcvok-algo-1-uoa0q  | 2024-11-27 23:53:57,408 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "fzavvkcvok-algo-1-uoa0q  | 2024-11-27 23:53:57,419 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "fzavvkcvok-algo-1-uoa0q  | 2024-11-27 23:53:57,426 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "fzavvkcvok-algo-1-uoa0q  | 2024-11-27 23:53:57,434 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "fzavvkcvok-algo-1-uoa0q  | 2024-11-27 23:53:57,435 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "fzavvkcvok-algo-1-uoa0q  | /opt/conda/bin/python3.8 -m pip install -r requirements.txt\n",
      "fzavvkcvok-algo-1-uoa0q  | Collecting smprof==0.3.334\n",
      "fzavvkcvok-algo-1-uoa0q  | Downloading https://smppy.s3.amazonaws.com/pytorch/cu113/smprof-0.3.334-cp38-cp38-linux_x86_64.whl (6.5 MB)\n",
      "fzavvkcvok-algo-1-uoa0q  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.5/6.5 MB 12.4 MB/s eta 0:00:00\n",
      "fzavvkcvok-algo-1-uoa0q  | \n",
      "fzavvkcvok-algo-1-uoa0q  | Collecting tensorboard==2.13.0\n",
      "fzavvkcvok-algo-1-uoa0q  | Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "fzavvkcvok-algo-1-uoa0q  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 19.4 MB/s eta 0:00:00\n",
      "fzavvkcvok-algo-1-uoa0q  | \n",
      "fzavvkcvok-algo-1-uoa0q  | Collecting google-auth<3,>=1.6.3\n",
      "fzavvkcvok-algo-1-uoa0q  | Downloading google_auth-2.36.0-py2.py3-none-any.whl (209 kB)\n",
      "fzavvkcvok-algo-1-uoa0q  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 209.5/209.5 kB 31.0 MB/s eta 0:00:00\n",
      "fzavvkcvok-algo-1-uoa0q  | \n",
      "fzavvkcvok-algo-1-uoa0q  | Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.13.0->-r requirements.txt (line 1)) (2.2.2)\n",
      "fzavvkcvok-algo-1-uoa0q  | Requirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.13.0->-r requirements.txt (line 1)) (3.20.2)\n",
      "fzavvkcvok-algo-1-uoa0q  | Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "fzavvkcvok-algo-1-uoa0q  | Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "fzavvkcvok-algo-1-uoa0q  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 38.1 MB/s eta 0:00:00\n",
      "fzavvkcvok-algo-1-uoa0q  | \n",
      "fzavvkcvok-algo-1-uoa0q  | Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.13.0->-r requirements.txt (line 1)) (65.6.3)\n",
      "fzavvkcvok-algo-1-uoa0q  | Collecting grpcio>=1.48.2\n",
      "fzavvkcvok-algo-1-uoa0q  | Downloading grpcio-1.68.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "fzavvkcvok-algo-1-uoa0q  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.9/5.9 MB 51.4 MB/s eta 0:00:00\n",
      "fzavvkcvok-algo-1-uoa0q  | \n",
      "fzavvkcvok-algo-1-uoa0q  | Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.13.0->-r requirements.txt (line 1)) (2.28.2)\n",
      "fzavvkcvok-algo-1-uoa0q  | Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "fzavvkcvok-algo-1-uoa0q  | Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "fzavvkcvok-algo-1-uoa0q  | Collecting absl-py>=0.4\n",
      "fzavvkcvok-algo-1-uoa0q  | Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "fzavvkcvok-algo-1-uoa0q  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.7/133.7 kB 24.5 MB/s eta 0:00:00\n",
      "fzavvkcvok-algo-1-uoa0q  | \n",
      "fzavvkcvok-algo-1-uoa0q  | Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.13.0->-r requirements.txt (line 1)) (0.38.4)\n",
      "fzavvkcvok-algo-1-uoa0q  | Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.13.0->-r requirements.txt (line 1)) (1.22.2)\n",
      "fzavvkcvok-algo-1-uoa0q  | Collecting markdown>=2.6.8\n",
      "fzavvkcvok-algo-1-uoa0q  | Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "fzavvkcvok-algo-1-uoa0q  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.3/106.3 kB 24.5 MB/s eta 0:00:00\n",
      "fzavvkcvok-algo-1-uoa0q  | \n",
      "fzavvkcvok-algo-1-uoa0q  | Collecting cachetools<6.0,>=2.0.0\n",
      "fzavvkcvok-algo-1-uoa0q  | Downloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "fzavvkcvok-algo-1-uoa0q  | Collecting pyasn1-modules>=0.2.1\n",
      "fzavvkcvok-algo-1-uoa0q  | Downloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "fzavvkcvok-algo-1-uoa0q  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.5/181.5 kB 27.9 MB/s eta 0:00:00\n",
      "fzavvkcvok-algo-1-uoa0q  | \n",
      "fzavvkcvok-algo-1-uoa0q  | Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.13.0->-r requirements.txt (line 1)) (4.7.2)\n",
      "fzavvkcvok-algo-1-uoa0q  | Collecting requests-oauthlib>=0.7.0\n",
      "fzavvkcvok-algo-1-uoa0q  | Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "fzavvkcvok-algo-1-uoa0q  | Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard==2.13.0->-r requirements.txt (line 1)) (4.13.0)\n",
      "fzavvkcvok-algo-1-uoa0q  | Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.13.0->-r requirements.txt (line 1)) (3.4)\n",
      "fzavvkcvok-algo-1-uoa0q  | Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.13.0->-r requirements.txt (line 1)) (1.26.14)\n",
      "fzavvkcvok-algo-1-uoa0q  | Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.13.0->-r requirements.txt (line 1)) (2022.12.7)\n",
      "fzavvkcvok-algo-1-uoa0q  | Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.13.0->-r requirements.txt (line 1)) (2.1.1)\n",
      "fzavvkcvok-algo-1-uoa0q  | Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard==2.13.0->-r requirements.txt (line 1)) (2.1.2)\n",
      "fzavvkcvok-algo-1-uoa0q  | Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==2.13.0->-r requirements.txt (line 1)) (3.13.0)\n",
      "fzavvkcvok-algo-1-uoa0q  | Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.13.0->-r requirements.txt (line 1)) (0.4.8)\n",
      "fzavvkcvok-algo-1-uoa0q  | Collecting oauthlib>=3.0.0\n",
      "fzavvkcvok-algo-1-uoa0q  | Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "fzavvkcvok-algo-1-uoa0q  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 41.8 MB/s eta 0:00:00\n",
      "fzavvkcvok-algo-1-uoa0q  | \n",
      "fzavvkcvok-algo-1-uoa0q  | Installing collected packages: tensorboard-data-server, smprof, pyasn1-modules, oauthlib, grpcio, cachetools, absl-py, requests-oauthlib, markdown, google-auth, google-auth-oauthlib, tensorboard\n",
      "fzavvkcvok-algo-1-uoa0q  | Successfully installed absl-py-2.1.0 cachetools-5.5.0 google-auth-2.36.0 google-auth-oauthlib-1.0.0 grpcio-1.68.0 markdown-3.7 oauthlib-3.2.2 pyasn1-modules-0.4.1 requests-oauthlib-2.0.0 smprof-0.3.334 tensorboard-2.13.0 tensorboard-data-server-0.7.2\n",
      "fzavvkcvok-algo-1-uoa0q  | WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "fzavvkcvok-algo-1-uoa0q  | [notice] A new release of pip is available: 23.0 -> 24.3.1\n",
      "fzavvkcvok-algo-1-uoa0q  | [notice] To update, run: pip install --upgrade pip\n",
      "fzavvkcvok-algo-1-uoa0q  | 2024-11-27 23:54:05,319 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "fzavvkcvok-algo-1-uoa0q  | 2024-11-27 23:54:05,319 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "fzavvkcvok-algo-1-uoa0q  | 2024-11-27 23:54:05,325 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "fzavvkcvok-algo-1-uoa0q  | 2024-11-27 23:54:05,329 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "fzavvkcvok-algo-1-uoa0q  | 2024-11-27 23:54:05,358 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "fzavvkcvok-algo-1-uoa0q  | 2024-11-27 23:54:05,366 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "fzavvkcvok-algo-1-uoa0q  | 2024-11-27 23:54:05,369 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "fzavvkcvok-algo-1-uoa0q  | 2024-11-27 23:54:05,379 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "fzavvkcvok-algo-1-uoa0q  | 2024-11-27 23:54:05,387 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "fzavvkcvok-algo-1-uoa0q  | 2024-11-27 23:54:05,389 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "fzavvkcvok-algo-1-uoa0q  | 2024-11-27 23:54:05,398 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "fzavvkcvok-algo-1-uoa0q  | 2024-11-27 23:54:05,403 sagemaker-training-toolkit INFO     Invoking user script\n",
      "fzavvkcvok-algo-1-uoa0q  | \n",
      "fzavvkcvok-algo-1-uoa0q  | Training Env:\n",
      "fzavvkcvok-algo-1-uoa0q  | \n",
      "fzavvkcvok-algo-1-uoa0q  | {\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"additional_framework_parameters\": {},\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"channel_input_dirs\": {},\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"current_host\": \"algo-1-uoa0q\",\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"current_instance_group\": \"homogeneousCluster\",\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"current_instance_group_hosts\": [],\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"current_instance_type\": \"local\",\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"distribution_hosts\": [\n",
      "fzavvkcvok-algo-1-uoa0q  |         \"algo-1-uoa0q\"\n",
      "fzavvkcvok-algo-1-uoa0q  |     ],\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"distribution_instance_groups\": [],\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"hosts\": [\n",
      "fzavvkcvok-algo-1-uoa0q  |         \"algo-1-uoa0q\"\n",
      "fzavvkcvok-algo-1-uoa0q  |     ],\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"hyperparameters\": {},\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"input_data_config\": {},\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"input_dir\": \"/opt/ml/input\",\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"instance_groups\": [],\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"instance_groups_dict\": {},\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"is_hetero\": false,\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"is_master\": true,\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"is_modelparallel_enabled\": null,\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"is_smddpmprun_installed\": false,\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"job_name\": \"Mounir-pytorch-mnist-profiler-2024-11-28-04-53-56-121\",\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"log_level\": 20,\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"master_hostname\": \"algo-1-uoa0q\",\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"model_dir\": \"/opt/ml/model\",\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"module_dir\": \"/opt/ml/code\",\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"module_name\": \"mnist_train\",\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"network_interface_name\": \"eth0\",\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"num_cpus\": 2,\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"num_gpus\": 0,\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"num_neurons\": 0,\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"output_dir\": \"/opt/ml/output\",\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"resource_config\": {\n",
      "fzavvkcvok-algo-1-uoa0q  |         \"current_host\": \"algo-1-uoa0q\",\n",
      "fzavvkcvok-algo-1-uoa0q  |         \"hosts\": [\n",
      "fzavvkcvok-algo-1-uoa0q  |             \"algo-1-uoa0q\"\n",
      "fzavvkcvok-algo-1-uoa0q  |         ]\n",
      "fzavvkcvok-algo-1-uoa0q  |     },\n",
      "fzavvkcvok-algo-1-uoa0q  |     \"user_entry_point\": \"mnist_train.py\"\n",
      "fzavvkcvok-algo-1-uoa0q  | }\n",
      "fzavvkcvok-algo-1-uoa0q  | \n",
      "fzavvkcvok-algo-1-uoa0q  | Environment variables:\n",
      "fzavvkcvok-algo-1-uoa0q  | \n",
      "fzavvkcvok-algo-1-uoa0q  | SM_HOSTS=[\"algo-1-uoa0q\"]\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_NETWORK_INTERFACE_NAME=eth0\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_HPS={}\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_USER_ENTRY_POINT=mnist_train.py\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_FRAMEWORK_PARAMS={}\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-uoa0q\",\"hosts\":[\"algo-1-uoa0q\"]}\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_INPUT_DATA_CONFIG={}\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_CHANNELS=[]\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_CURRENT_HOST=algo-1-uoa0q\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_CURRENT_INSTANCE_TYPE=local\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_CURRENT_INSTANCE_GROUP_HOSTS=[]\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_INSTANCE_GROUPS=[]\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_INSTANCE_GROUPS_DICT={}\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_IS_HETERO=false\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_MODULE_NAME=mnist_train\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_LOG_LEVEL=20\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_INPUT_DIR=/opt/ml/input\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_OUTPUT_DIR=/opt/ml/output\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_NUM_CPUS=2\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_NUM_GPUS=0\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_NUM_NEURONS=0\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_MODEL_DIR=/opt/ml/model\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_MODULE_DIR=/opt/ml/code\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1-uoa0q\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[],\"current_instance_type\":\"local\",\"distribution_hosts\":[\"algo-1-uoa0q\"],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-uoa0q\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[],\"instance_groups_dict\":{},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"Mounir-pytorch-mnist-profiler-2024-11-28-04-53-56-121\",\"log_level\":20,\"master_hostname\":\"algo-1-uoa0q\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/code\",\"module_name\":\"mnist_train\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-uoa0q\",\"hosts\":[\"algo-1-uoa0q\"]},\"user_entry_point\":\"mnist_train.py\"}\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_USER_ARGS=[]\n",
      "fzavvkcvok-algo-1-uoa0q  | SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "fzavvkcvok-algo-1-uoa0q  | PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\n",
      "fzavvkcvok-algo-1-uoa0q  | \n",
      "fzavvkcvok-algo-1-uoa0q  | Invoking script with the following command:\n",
      "fzavvkcvok-algo-1-uoa0q  | \n",
      "fzavvkcvok-algo-1-uoa0q  | /opt/conda/bin/python3.8 mnist_train.py\n",
      "fzavvkcvok-algo-1-uoa0q  | \n",
      "fzavvkcvok-algo-1-uoa0q  | \n",
      "fzavvkcvok-algo-1-uoa0q  | 2024-11-27 23:54:06,083 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\n",
      "fzavvkcvok-algo-1-uoa0q  | Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-images-idx3-ubyte.gz\n",
      "fzavvkcvok-algo-1-uoa0q  | Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-images-idx3-ubyte.gz to /tmp/data/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "fzavvkcvok-algo-1-uoa0q  | 0%|          | 0/9912422 [00:00<?, ?it/s]\n",
      "fzavvkcvok-algo-1-uoa0q  | 1%|          | 52224/9912422 [00:00<00:21, 453324.72it/s]\n",
      "fzavvkcvok-algo-1-uoa0q  | 3%|▎         | 261120/9912422 [00:00<00:07, 1235655.33it/s]\n",
      "fzavvkcvok-algo-1-uoa0q  | 12%|█▏        | 1148928/9912422 [00:00<00:02, 4123158.58it/s]\n",
      "fzavvkcvok-algo-1-uoa0q  | 47%|████▋     | 4703232/9912422 [00:00<00:00, 14403897.22it/s]\n",
      "fzavvkcvok-algo-1-uoa0q  | 87%|████████▋ | 8619008/9912422 [00:00<00:00, 21192290.36it/s]\n",
      "fzavvkcvok-algo-1-uoa0q  | 9913344it [00:00, 16566717.53it/s]\n",
      "fzavvkcvok-algo-1-uoa0q  | Extracting /tmp/data/MNIST/raw/train-images-idx3-ubyte.gz to /tmp/data/MNIST/raw\n",
      "fzavvkcvok-algo-1-uoa0q  | \n",
      "fzavvkcvok-algo-1-uoa0q  | Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-labels-idx1-ubyte.gz\n",
      "fzavvkcvok-algo-1-uoa0q  | \n",
      "fzavvkcvok-algo-1-uoa0q  | Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-labels-idx1-ubyte.gz to /tmp/data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "fzavvkcvok-algo-1-uoa0q  | 0%|          | 0/28881 [00:00<?, ?it/s]\n",
      "fzavvkcvok-algo-1-uoa0q  | 29696it [00:00, 470466.23it/s]\n",
      "fzavvkcvok-algo-1-uoa0q  | Extracting /tmp/data/MNIST/raw/train-labels-idx1-ubyte.gz to /tmp/data/MNIST/raw\n",
      "fzavvkcvok-algo-1-uoa0q  | Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-images-idx3-ubyte.gz\n",
      "fzavvkcvok-algo-1-uoa0q  | Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-images-idx3-ubyte.gz to /tmp/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "fzavvkcvok-algo-1-uoa0q  | 0%|          | 0/1648877 [00:00<?, ?it/s]\n",
      "fzavvkcvok-algo-1-uoa0q  | 4%|▍         | 69632/1648877 [00:00<00:02, 616640.89it/s]\n",
      "fzavvkcvok-algo-1-uoa0q  | 18%|█▊        | 295936/1648877 [00:00<00:00, 1431474.74it/s]\n",
      "fzavvkcvok-algo-1-uoa0q  | 77%|███████▋  | 1270784/1648877 [00:00<00:00, 4711405.55it/s]\n",
      "fzavvkcvok-algo-1-uoa0q  | 1649664it [00:00, 4806192.87it/s]\n",
      "fzavvkcvok-algo-1-uoa0q  | Extracting /tmp/data/MNIST/raw/t10k-images-idx3-ubyte.gz to /tmp/data/MNIST/raw\n",
      "fzavvkcvok-algo-1-uoa0q  | Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-labels-idx1-ubyte.gz\n",
      "fzavvkcvok-algo-1-uoa0q  | Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-labels-idx1-ubyte.gz to /tmp/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "fzavvkcvok-algo-1-uoa0q  | 0%|          | 0/4542 [00:00<?, ?it/s]\n",
      "fzavvkcvok-algo-1-uoa0q  | 5120it [00:00, 2761681.65it/s]\n",
      "fzavvkcvok-algo-1-uoa0q  | Extracting /tmp/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/data/MNIST/raw\n",
      "fzavvkcvok-algo-1-uoa0q  | \n",
      "fzavvkcvok-algo-1-uoa0q  | LOG(WARNING) [ 23:54:12.284 ] @ smprof/smprof.cxx:330| Setting output file in SageMaker environment is not allowed!\n",
      "fzavvkcvok-algo-1-uoa0q  | LOG(INFO) [ 23:54:12.284 ] @ smprof/smprof.cxx:384| CUDA capture enabled\n",
      "fzavvkcvok-algo-1-uoa0q  | LOG(ERROR) [ 23:54:12.284 ] @ smprof/io/smpfilewriter.cxx:76| Failed to open file /opt/ml/output/profiler/framework/smprofiler_algo-1-uoa0q_54_1732769652284931901_0.smpraw error was No such file or directory\n",
      "fzavvkcvok-algo-1-uoa0q  | [2024-11-27 23:54:12.369 algo-1-uoa0q:54 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "fzavvkcvok-algo-1-uoa0q  | /opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "fzavvkcvok-algo-1-uoa0q  | /opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "fzavvkcvok-algo-1-uoa0q  | [2024-11-27 23:54:13.013 algo-1-uoa0q:54 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "fzavvkcvok-algo-1-uoa0q  | /opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1320: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "fzavvkcvok-algo-1-uoa0q  |   warnings.warn(warn_msg)\n",
      "fzavvkcvok-algo-1-uoa0q  | Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.307322\n",
      "fzavvkcvok-algo-1-uoa0q  | Train Epoch: 1 [1280/60000 (2%)]\tLoss: 1.744434\n",
      "fzavvkcvok-algo-1-uoa0q  | Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.424744\n",
      "fzavvkcvok-algo-1-uoa0q  | Train Epoch: 1 [3840/60000 (6%)]\tLoss: 1.307156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:creating /tmp/tmphh2b8mx2/artifacts/output/data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SageMaker/.conda/envs/fm-training_prod/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SageMaker/.conda/envs/fm-training_prod/lib/python3.10/site-packages/sagemaker/estimator.py:1343\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_for_training(job_name\u001b[38;5;241m=\u001b[39mjob_name)\n\u001b[1;32m   1342\u001b[0m experiment_config \u001b[38;5;241m=\u001b[39m check_and_get_run_experiment_config(experiment_config)\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job \u001b[38;5;241m=\u001b[39m \u001b[43m_TrainingJob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_new\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n",
      "File \u001b[0;32m~/SageMaker/.conda/envs/fm-training_prod/lib/python3.10/site-packages/sagemaker/estimator.py:2451\u001b[0m, in \u001b[0;36m_TrainingJob.start_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   2448\u001b[0m train_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_train_args(estimator, inputs, experiment_config)\n\u001b[1;32m   2450\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain args after processing defaults: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, train_args)\n\u001b[0;32m-> 2451\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(estimator\u001b[38;5;241m.\u001b[39msagemaker_session, estimator\u001b[38;5;241m.\u001b[39m_current_job_name)\n",
      "File \u001b[0;32m~/SageMaker/.conda/envs/fm-training_prod/lib/python3.10/site-packages/sagemaker/session.py:1006\u001b[0m, in \u001b[0;36mSession.train\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, training_image_config, infra_check_config, container_entry_point, container_arguments, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy, remote_debug_config, session_chaining_config)\u001b[0m\n\u001b[1;32m   1003\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(request, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m   1004\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client\u001b[38;5;241m.\u001b[39mcreate_training_job(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest)\n\u001b[0;32m-> 1006\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_intercept_create_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SageMaker/.conda/envs/fm-training_prod/lib/python3.10/site-packages/sagemaker/session.py:6458\u001b[0m, in \u001b[0;36mSession._intercept_create_request\u001b[0;34m(self, request, create, func_name)\u001b[0m\n\u001b[1;32m   6441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_intercept_create_request\u001b[39m(\n\u001b[1;32m   6442\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   6443\u001b[0m     request: typing\u001b[38;5;241m.\u001b[39mDict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6446\u001b[0m     \u001b[38;5;66;03m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[1;32m   6447\u001b[0m ):\n\u001b[1;32m   6448\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This function intercepts the create job request.\u001b[39;00m\n\u001b[1;32m   6449\u001b[0m \n\u001b[1;32m   6450\u001b[0m \u001b[38;5;124;03m    PipelineSession inherits this Session class and will override\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6456\u001b[0m \u001b[38;5;124;03m        func_name (str): the name of the function needed intercepting\u001b[39;00m\n\u001b[1;32m   6457\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 6458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SageMaker/.conda/envs/fm-training_prod/lib/python3.10/site-packages/sagemaker/session.py:1004\u001b[0m, in \u001b[0;36mSession.train.<locals>.submit\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m   1002\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating training-job with name: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, job_name)\n\u001b[1;32m   1003\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(request, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m-> 1004\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_training_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SageMaker/.conda/envs/fm-training_prod/lib/python3.10/site-packages/sagemaker/local/local_session.py:212\u001b[0m, in \u001b[0;36mLocalSagemakerClient.create_training_job\u001b[0;34m(self, TrainingJobName, AlgorithmSpecification, OutputDataConfig, ResourceConfig, InputDataConfig, Environment, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m hyperparameters \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHyperParameters\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHyperParameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    211\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training job\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 212\u001b[0m \u001b[43mtraining_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mInputDataConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOutputDataConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEnvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTrainingJobName\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m LocalSagemakerClient\u001b[38;5;241m.\u001b[39m_training_jobs[TrainingJobName] \u001b[38;5;241m=\u001b[39m training_job\n",
      "File \u001b[0;32m~/SageMaker/.conda/envs/fm-training_prod/lib/python3.10/site-packages/sagemaker/local/entities.py:246\u001b[0m, in \u001b[0;36m_LocalTrainingJob.start\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, environment, job_name)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_TRAINING\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment \u001b[38;5;241m=\u001b[39m environment\n\u001b[0;32m--> 246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_artifacts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_data_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_data_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_COMPLETED\n",
      "File \u001b[0;32m~/SageMaker/.conda/envs/fm-training_prod/lib/python3.10/site-packages/sagemaker/local/image.py:314\u001b[0m, in \u001b[0;36m_SageMakerContainer.train\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, environment, job_name)\u001b[0m\n\u001b[1;32m    309\u001b[0m process \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(\n\u001b[1;32m    310\u001b[0m     compose_command, stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE, stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mSTDOUT\n\u001b[1;32m    311\u001b[0m )\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 314\u001b[0m     \u001b[43m_stream_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     artifacts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve_artifacts(compose_data, output_data_config, job_name)\n",
      "File \u001b[0;32m~/SageMaker/.conda/envs/fm-training_prod/lib/python3.10/site-packages/sagemaker/local/image.py:1016\u001b[0m, in \u001b[0;36m_stream_output\u001b[0;34m(process)\u001b[0m\n\u001b[1;32m   1013\u001b[0m exit_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m exit_code \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1016\u001b[0m     stdout \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1017\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mwrite(stdout)\n\u001b[1;32m   1018\u001b[0m     exit_code \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5df9a55-b008-4c4e-8da5-0a1c8a55f9b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker import ProfilerConfig, Profiler\n",
    "\n",
    "import sagemaker\n",
    "\n",
    "# Creates a SageMaker session to interact with the AWS environment.\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Fetches the AWS region name for the current session.\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "# Retrieves the default Amazon S3 bucket for the SageMaker session.\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "\n",
    "# Specifies a folder prefix within the bucket for organizing resources.\n",
    "prefix = \"sagemaker/DEMO-sagemaker-profiler\"\n",
    "\n",
    "# Fetches the IAM role ARN associated with the SageMaker notebook or instance.\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "instance_type=\"ml.g4dn.12xlarge\"\n",
    "\n",
    "estimator = PyTorch(\n",
    "    base_job_name=\"Pytorch-mnist-profiler\",\n",
    "    source_dir=\"Mnist_code\",\n",
    "    entry_point=\"mnist_train.py\",\n",
    "    role=role,\n",
    "    framework_version=\"1.11.0\",\n",
    "    py_version=\"py38\",\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    profiler_config=profiler_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7cbd42cc-7a29-40b4-816b-4c9ded436e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: Mounir-pytorch-mnist-profiler-2024-11-28-05-44-00-789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-28 05:44:01 Starting - Starting the training job...\n",
      "2024-11-28 05:44:26 Starting - Preparing the instances for trainingDetailedProfilerProcessingJobConfig-2024-11-28-05-44-00-862: InProgress\n",
      "...\n",
      "2024-11-28 05:44:52 Downloading - Downloading input data...\n",
      "2024-11-28 05:45:26 Downloading - Downloading the training image.....................\n",
      "2024-11-28 05:48:54 Training - Training image download completed. Training in progress....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-11-28 00:49:21,446 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-11-28 00:49:21,483 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-11-28 00:49:21,496 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-11-28 00:49:21,498 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-11-28 00:49:21,723 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting smprof==0.3.334\u001b[0m\n",
      "\u001b[34mDownloading https://smppy.s3.amazonaws.com/pytorch/cu113/smprof-0.3.334-cp38-cp38-linux_x86_64.whl (6.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.5/6.5 MB 13.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tensorboard==2.13.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 17.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.13.0->-r requirements.txt (line 1)) (0.38.4)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.7-py3-none-any.whl (106 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.3/106.3 kB 37.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.48.2\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.68.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.9/5.9 MB 44.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.13.0->-r requirements.txt (line 1)) (3.20.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.13.0->-r requirements.txt (line 1)) (65.6.3)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.8.0,>=0.7.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 69.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4\u001b[0m\n",
      "\u001b[34mDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.7/133.7 kB 43.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.13.0->-r requirements.txt (line 1)) (2.28.2)\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<1.1,>=0.5\u001b[0m\n",
      "\u001b[34mDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.13.0->-r requirements.txt (line 1)) (2.2.3)\u001b[0m\n",
      "\u001b[34mCollecting google-auth<3,>=1.6.3\u001b[0m\n",
      "\u001b[34mDownloading google_auth-2.36.0-py2.py3-none-any.whl (209 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 209.5/209.5 kB 56.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.13.0->-r requirements.txt (line 1)) (1.22.2)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<6.0,>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.13.0->-r requirements.txt (line 1)) (4.7.2)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\u001b[0m\n",
      "\u001b[34mDownloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.5/181.5 kB 48.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\u001b[0m\n",
      "\u001b[34mDownloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard==2.13.0->-r requirements.txt (line 1)) (4.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.13.0->-r requirements.txt (line 1)) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.13.0->-r requirements.txt (line 1)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.13.0->-r requirements.txt (line 1)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.13.0->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard==2.13.0->-r requirements.txt (line 1)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==2.13.0->-r requirements.txt (line 1)) (3.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.13.0->-r requirements.txt (line 1)) (0.4.8)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0\u001b[0m\n",
      "\u001b[34mDownloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 20.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tensorboard-data-server, smprof, pyasn1-modules, oauthlib, grpcio, cachetools, absl-py, requests-oauthlib, markdown, google-auth, google-auth-oauthlib, tensorboard\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-2.1.0 cachetools-5.5.0 google-auth-2.36.0 google-auth-oauthlib-1.0.0 grpcio-1.68.0 markdown-3.7 oauthlib-3.2.2 pyasn1-modules-0.4.1 requests-oauthlib-2.0.0 smprof-0.3.334 tensorboard-2.13.0 tensorboard-data-server-0.7.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 24.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-11-28 00:49:28,832 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-11-28 00:49:28,832 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-11-28 00:49:28,884 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-11-28 00:49:28,936 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-11-28 00:49:28,989 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-11-28 00:49:29,003 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"Mounir-pytorch-mnist-profiler-2024-11-28-05-44-00-789\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-785721480234/Mounir-pytorch-mnist-profiler-2024-11-28-05-44-00-789/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist_train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist_train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mnist_train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mnist_train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-785721480234/Mounir-pytorch-mnist-profiler-2024-11-28-05-44-00-789/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"Mounir-pytorch-mnist-profiler-2024-11-28-05-44-00-789\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-785721480234/Mounir-pytorch-mnist-profiler-2024-11-28-05-44-00-789/source/sourcedir.tar.gz\",\"module_name\":\"mnist_train\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist_train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 mnist_train.py\u001b[0m\n",
      "\u001b[34m2024-11-28 00:49:30,201 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mDownloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-images-idx3-ubyte.gz to /tmp/data/MNIST/raw/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/9912422 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 71680/9912422 [00:00<00:15, 625971.21it/s]\u001b[0m\n",
      "\u001b[34m4%|▎         | 367616/9912422 [00:00<00:05, 1753278.65it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 1551360/9912422 [00:00<00:01, 5583129.50it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 5549056/9912422 [00:00<00:00, 16862126.61it/s]\u001b[0m\n",
      "\u001b[34m9913344it [00:00, 18419225.65it/s]\u001b[0m\n",
      "\u001b[34mExtracting /tmp/data/MNIST/raw/train-images-idx3-ubyte.gz to /tmp/data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-labels-idx1-ubyte.gz to /tmp/data/MNIST/raw/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/28881 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m29696it [00:00, 522427.59it/s]\u001b[0m\n",
      "\u001b[34mExtracting /tmp/data/MNIST/raw/train-labels-idx1-ubyte.gz to /tmp/data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-images-idx3-ubyte.gz to /tmp/data/MNIST/raw/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/1648877 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 69632/1648877 [00:00<00:02, 576863.23it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 330752/1648877 [00:00<00:00, 1509524.48it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1392640/1648877 [00:00<00:00, 4831052.15it/s]\u001b[0m\n",
      "\u001b[34m1649664it [00:00, 4523465.48it/s]\u001b[0m\n",
      "\u001b[34mExtracting /tmp/data/MNIST/raw/t10k-images-idx3-ubyte.gz to /tmp/data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-labels-idx1-ubyte.gz to /tmp/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/4542 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m5120it [00:00, 6936316.69it/s]\u001b[0m\n",
      "\u001b[34mExtracting /tmp/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/data/MNIST/raw\u001b[0m\n",
      "\u001b[34mLOG(WARNING) [ 00:49:39.716 ] @ smprof/smprof.cxx:330| Setting output file in SageMaker environment is not allowed!\u001b[0m\n",
      "\u001b[34mLOG(INFO) [ 00:49:39.717 ] @ smprof/smprof.cxx:384| CUDA capture enabled\u001b[0m\n",
      "\u001b[34mLOG(INFO) [ 00:49:40.697 ] @ smprof/smprof.cxx:182| Status check failed Couldn't find any library with annotations status= Failed to find hook functions\u001b[0m\n",
      "\u001b[34m[2024-11-28 00:49:40.970 algo-1:112 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[2024-11-28 00:49:41.490 algo-1:112 INFO profiler_config_parser.py:111] Using config at /opt/ml/input/config/profilerconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-11-28 00:49:41.492 algo-1:112 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-11-28 00:49:41.492 algo-1:112 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2024-11-28 00:49:41.493 algo-1:112 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-11-28 00:49:41.493 algo-1:112 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1320: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [0/60000 (0%)]#011Loss: 2.307185\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [1280/60000 (2%)]#011Loss: 2.038476\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [2560/60000 (4%)]#011Loss: 1.461991\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [3840/60000 (6%)]#011Loss: 1.356992\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [5120/60000 (9%)]#011Loss: 1.637100\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [6400/60000 (11%)]#011Loss: 1.443729\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [7680/60000 (13%)]#011Loss: 1.273724\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [8960/60000 (15%)]#011Loss: 1.310644\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [10240/60000 (17%)]#011Loss: 1.422111\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [11520/60000 (19%)]#011Loss: 1.387985\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [12800/60000 (21%)]#011Loss: 1.315669\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [14080/60000 (23%)]#011Loss: 1.372217\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [15360/60000 (26%)]#011Loss: 1.146695\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [16640/60000 (28%)]#011Loss: 1.263266\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [17920/60000 (30%)]#011Loss: 1.326112\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [19200/60000 (32%)]#011Loss: 1.339407\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [20480/60000 (34%)]#011Loss: 1.170430\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [21760/60000 (36%)]#011Loss: 0.955134\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [23040/60000 (38%)]#011Loss: 1.280869\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [24320/60000 (41%)]#011Loss: 1.130534\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [25600/60000 (43%)]#011Loss: 1.214323\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [26880/60000 (45%)]#011Loss: 1.225749\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [28160/60000 (47%)]#011Loss: 1.115742\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [29440/60000 (49%)]#011Loss: 1.160695\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [30720/60000 (51%)]#011Loss: 1.213804\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [32000/60000 (53%)]#011Loss: 1.281062\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [33280/60000 (55%)]#011Loss: 1.282777\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [34560/60000 (58%)]#011Loss: 1.317003\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [35840/60000 (60%)]#011Loss: 1.113842\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [37120/60000 (62%)]#011Loss: 1.229130\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [38400/60000 (64%)]#011Loss: 1.203146\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [39680/60000 (66%)]#011Loss: 1.277248\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [40960/60000 (68%)]#011Loss: 1.217082\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [42240/60000 (70%)]#011Loss: 1.097304\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [43520/60000 (72%)]#011Loss: 1.198961\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [44800/60000 (75%)]#011Loss: 1.247321\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [46080/60000 (77%)]#011Loss: 1.089180\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [47360/60000 (79%)]#011Loss: 1.322021\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [48640/60000 (81%)]#011Loss: 1.231584\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [49920/60000 (83%)]#011Loss: 1.159703\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [51200/60000 (85%)]#011Loss: 1.276120\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [52480/60000 (87%)]#011Loss: 1.256597\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [53760/60000 (90%)]#011Loss: 1.304814\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [55040/60000 (92%)]#011Loss: 1.245703\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [56320/60000 (94%)]#011Loss: 1.117022\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [57600/60000 (96%)]#011Loss: 1.144826\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [58880/60000 (98%)]#011Loss: 1.098785\u001b[0m\n",
      "\u001b[34mLOG(INFO) [ 00:49:59.656 ] @ smprof/smprof.cxx:212| Status check failed Annotator flush failed! status= Buffer is Empty\u001b[0m\n",
      "\u001b[34mLOG(INFO) [ 00:49:59.658 ] @ smprof/smprof.cxx:214| Status check failed Can't find libraries with annotations status= Failed to find hook functions\u001b[0m\n",
      "\u001b[34mLOG(INFO) [ 00:49:59.896 ] @ smprof/io/smpfilewriter.cxx:255| Writer thread spin duration= 0us, number of buckets= 2, number of records= 751927, total time= 0.037s, work time= 0.037s, average buckets/s=52.9647s, effective buckets/s=52.9647, max uncompressed bucket size =67108768, max compressed bucket size= 10105725, total written bytes= 10811483, total uncompressed bytes= 70669984, compression ratio= 6.53657, average compressed bandwidth= 273.049 MB/s, effective compressed bandwidth= 273.049 MB/s,  average uncompressed bandwidth= 1784.81 MB/s. Last file rotation id= 0\u001b[0m\n",
      "\u001b[34mLOG(INFO) [ 00:49:59.905 ] @ smprof/io/smpfilewriter.cxx:321| Rename /opt/ml/output/profiler/framework/smprofiler_algo-1_112_1732772979717211118_0.smpraw.tmp to /opt/ml/output/profiler/framework/smprofiler_algo-1_112_1732772979717211118_0.smpraw\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.1329, Accuracy: 9801/10000 (98%)\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [0/60000 (0%)]#011Loss: 1.315920\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [1280/60000 (2%)]#011Loss: 1.251150\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [2560/60000 (4%)]#011Loss: 1.030594\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [3840/60000 (6%)]#011Loss: 1.213015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [5120/60000 (9%)]#011Loss: 1.162225\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [6400/60000 (11%)]#011Loss: 1.127771\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [7680/60000 (13%)]#011Loss: 1.211349\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [8960/60000 (15%)]#011Loss: 1.399052\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [10240/60000 (17%)]#011Loss: 1.243241\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [11520/60000 (19%)]#011Loss: 1.327017\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [12800/60000 (21%)]#011Loss: 1.175729\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [14080/60000 (23%)]#011Loss: 1.224938\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [15360/60000 (26%)]#011Loss: 1.330936\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [16640/60000 (28%)]#011Loss: 1.192920\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [17920/60000 (30%)]#011Loss: 1.229164\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [19200/60000 (32%)]#011Loss: 1.193713\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [20480/60000 (34%)]#011Loss: 1.206745\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [21760/60000 (36%)]#011Loss: 1.038846\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [23040/60000 (38%)]#011Loss: 1.237683\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [24320/60000 (41%)]#011Loss: 1.331258\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [25600/60000 (43%)]#011Loss: 1.257965\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [26880/60000 (45%)]#011Loss: 1.371259\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [28160/60000 (47%)]#011Loss: 1.259075\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [29440/60000 (49%)]#011Loss: 1.071884\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [30720/60000 (51%)]#011Loss: 1.215861\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [32000/60000 (53%)]#011Loss: 1.133658\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [33280/60000 (55%)]#011Loss: 1.188007\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [34560/60000 (58%)]#011Loss: 1.277402\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [35840/60000 (60%)]#011Loss: 1.229454\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [37120/60000 (62%)]#011Loss: 1.027206\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [38400/60000 (64%)]#011Loss: 1.153868\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [39680/60000 (66%)]#011Loss: 1.221731\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [40960/60000 (68%)]#011Loss: 1.106808\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [42240/60000 (70%)]#011Loss: 1.375547\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [43520/60000 (72%)]#011Loss: 1.319487\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [44800/60000 (75%)]#011Loss: 1.099444\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [46080/60000 (77%)]#011Loss: 1.238422\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [47360/60000 (79%)]#011Loss: 0.990712\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [48640/60000 (81%)]#011Loss: 1.267585\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [49920/60000 (83%)]#011Loss: 0.996758\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [51200/60000 (85%)]#011Loss: 1.251976\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [52480/60000 (87%)]#011Loss: 1.249985\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [53760/60000 (90%)]#011Loss: 1.159679\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [55040/60000 (92%)]#011Loss: 1.101713\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [56320/60000 (94%)]#011Loss: 1.201797\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [57600/60000 (96%)]#011Loss: 1.241517\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [58880/60000 (98%)]#011Loss: 1.100271\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0799, Accuracy: 9843/10000 (98%)\u001b[0m\n",
      "\n",
      "2024-11-28 05:50:32 Uploading - Uploading generated training model\n",
      "2024-11-28 05:50:32 Completed - Training job completed\n",
      "\u001b[34mStalled 0s while waiting for a buffer\u001b[0m\n",
      "\u001b[34mLOG(INFO) [ 00:50:19.419 ] @ smprof/io/smpfilewriter.cxx:255| Writer thread spin duration= 0us, number of buckets= 1, number of records= 4690, total time= 0s, work time= 0s, average buckets/s=1e+06s, effective buckets/s=1e+06, max uncompressed bucket size =179304, max compressed bucket size= 41343, total written bytes= 41375, total uncompressed bytes= 179328, compression ratio= 4.33421, average compressed bandwidth= 39458.3 MB/s, effective compressed bandwidth= 39458.3 MB/s,  average uncompressed bandwidth= 171021 MB/s. Last file rotation id= 0\u001b[0m\n",
      "\u001b[34mLOG(INFO) [ 00:50:19.420 ] @ smprof/io/smpfilewriter.cxx:321| Rename /opt/ml/output/profiler/framework/smprofiler_algo-1_112_1732773002023118537_0.smpraw.tmp to /opt/ml/output/profiler/framework/smprofiler_algo-1_112_1732773002023118537_0.smpraw\u001b[0m\n",
      "\u001b[34m2024-11-28 00:50:19,785 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-11-28 00:50:19,786 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-11-28 00:50:19,786 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 341\n",
      "Billable seconds: 341\n"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_fm-training_prod",
   "language": "python",
   "name": "conda_fm-training_prod"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
